{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>A python library to prepare asynchronous time series datasets.</p>"},{"location":"#motivation","title":"Motivation","text":""},{"location":"#disclaimer-on-datasets","title":"Disclaimer on Datasets","text":"<p>This is a utility library that downloads and prepares public datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have license to use the dataset. It is your responsibility to determine whether you have permission to use the dataset under the dataset's license.</p> <p>If you're a dataset owner and wish to update any part of it, or do not want your dataset to be included in this library, please get in touch through a GitHub issue. Thanks for your contribution to the ML community!</p>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>aroma</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>aroma</code> to a new version will possibly break any code that was using the old version of <code>aroma</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>aroma</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"get_started/","title":"Get Started","text":"<p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-pip-recommended","title":"Installing with <code>pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>pip install aroma\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>aroma</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>pip install 'aroma[all]'\n</code></pre> <p>This command installs additional packages like <code>gdown</code>. It is also possible to install the optional packages manually or to select the packages to install. In the following example, only <code>gdown</code> is installed:</p> <pre><code>pip install aroma gdown\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>aroma</code> from source, you can follow the steps below. First, you will need to install <code>poetry</code>. <code>poetry</code> is used to manage and install the dependencies. If <code>poetry</code> is already installed on your machine, you can skip this step. There are several ways to install <code>poetry</code> so you can use the one that you prefer. You can check the <code>poetry</code> installation by running the following command:</p> <pre><code>poetry --version\n</code></pre> <p>Then, you can clone the git repository:</p> <pre><code>git clone git@github.com:durandtibo/aroma.git\n</code></pre> <p>It is recommended to create a Python 3.9+ virtual environment. This step is optional so you can skip it. To create a virtual environment, you can use the following command:</p> <pre><code>make conda\n</code></pre> <p>It automatically creates a conda virtual environment. When the virtual environment is created, you can activate it with the following command:</p> <pre><code>conda activate aroma\n</code></pre> <p>This example uses <code>conda</code> to create a virtual environment, but you can use other tools or configurations. Then, you should install the required package to use <code>aroma</code> with the following command:</p> <pre><code>make install\n</code></pre> <p>This command will install all the required packages. You can also use this command to update the required packages. This command will check if there is a more recent package available and will install it. Finally, you can test the installation with the following command:</p> <pre><code>make unit-test-cov\n</code></pre>"},{"location":"datasets/breakfast/","title":"Breakfast","text":""},{"location":"datasets/breakfast/#information-about-the-dataset","title":"Information about the dataset","text":"<p>This dataset contains 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. Overall, \u223c77 hours of video (&gt; 4 million frames) are recorded. The cameras used were webcams, standard industry cameras (Prosilica GE680C) as well as a stereo camera (BumbleBee , Pointgrey, Inc). To balance out viewpoints, we also mirrored videos recorded from laterally-positioned cameras. To reduce the overall amount of data, all videos were down-sampled to a resolution of 320\u00d7240 pixels with a frame rate of 15 fps. The number of cameras used varied from location to location (n = 3 \u2212 5). The cameras were uncalibrated and the position of the cameras changes based on the location.</p> <ul> <li>Project   webpage: https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/</li> <li>Research paper:   Kuehne, Arslan, and Serre.   The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities.   CVPR 2014.</li> </ul>"},{"location":"datasets/breakfast/#download-data","title":"Download data","text":"<p>The annotation data are stored in a Google Drive. You have to follow the instructions in the Download section to download the data.</p> <p>The annotation data used for the action prediction task are in the <code>segmentation_coarse.tar.gz</code> and <code>segmentation_fine.tar.gz</code> files. Then, you have to extract the files. It is possible to use the function <code>download_annotations</code> to automatically download the data:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from aroma.datasets.breakfast import download_annotations\n&gt;&gt;&gt; download_annotations(Path(\"/path/to/data/breakfast/\"))\n&gt;&gt;&gt; list(path.iterdir())\n[PosixPath('/path/to/data/breakfast/segmentation_coarse'),\n PosixPath('/path/to/data/breakfast/segmentation_fine')]\n</code></pre> <p>The remaining of the documentation assumes the data are stored in the directory <code>/path/to/data/breakfast/</code>.</p>"},{"location":"datasets/breakfast/#action-prediction-task","title":"Action prediction task","text":"<p>This section explains how to prepare the data for the action prediction task.</p>"},{"location":"datasets/breakfast/#get-the-event-data","title":"Get the event data","text":"<p>After the data are downloaded, you can get the event sequences by using the <code>load_event_data</code> function. This function returns the data and metadata. The following example shows how to get the event sequences by using the coarse annotations:</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.breakfast import load_event_data\ndata, metadata = load_event_data(Path(\"/path/to/data/breakfast/segmentation_coarse\"))\nprint(data.summary())\nprint(data)\nprint(metadata)\n</code></pre> <p>The output should look like:</p> <pre><code>BatchDict(\n  (action_index) BatchedTensorSeq(dtype=torch.int64, shape=torch.Size([508, 25]), device=cpu, batch_dim=0, seq_dim=1)\n  (cooking_activity) BatchList(batch_size=508)\n  (end_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([508, 25, 1]), device=cpu, batch_dim=0, seq_dim=1)\n  (person_id) BatchList(batch_size=508)\n  (start_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([508, 25, 1]), device=cpu, batch_dim=0, seq_dim=1)\n)\n\nBatchDict(\n  (action_index) tensor([[ 0, 16, 21,  ..., -1, -1, -1],\n            [ 0, 21,  1,  ..., -1, -1, -1],\n            [ 0, 16, 21,  ..., -1, -1, -1],\n            ...,\n            [ 0, 25, 23,  ..., -1, -1, -1],\n            [ 0, 13, 23,  ..., -1, -1, -1],\n            [ 0, 13, 23,  ..., -1, -1, -1]], batch_dim=0, seq_dim=1)\n  (cooking_activity) BatchList(data=['cereals', 'cereals', 'cereals', ..., 'tea'])\n  (end_time) tensor([[[ 30.],\n             [150.],\n             [428.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[ 55.],\n             [233.],\n             [405.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[ 13.],\n             [246.],\n             [624.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            ...,\n\n            [[448.],\n             [558.],\n             [754.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[ 11.],\n             [101.],\n             [316.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[ 37.],\n             [ 92.],\n             [229.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]]], batch_dim=0, seq_dim=1)\n  (person_id) BatchList(data=['P03', 'P04', 'P05', ..., 'P54'])\n  (start_time) tensor([[[  1.],\n             [ 31.],\n             [151.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[  1.],\n             [ 56.],\n             [234.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[  1.],\n             [ 14.],\n             [247.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            ...,\n\n            [[  1.],\n             [449.],\n             [559.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[  1.],\n             [ 12.],\n             [102.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[  1.],\n             [ 38.],\n             [ 93.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]]], batch_dim=0, seq_dim=1)\n)\n\n{'action_vocab': Vocabulary(\n  counter=Counter({'SIL': 1016, 'pour_milk': 199, 'cut_fruit': 176, ..., 'stir_tea': 2}),\n  index_to_token=('SIL', 'pour_milk', 'cut_fruit', 'crack_egg', ..., 'stir_tea'),\n  token_to_index={'SIL': 0, 'pour_milk': 1, 'cut_fruit': 2, ..., 'stir_tea': 47},\n)}\n</code></pre> <p>It is also possible to use the function <code>fetch_event_data</code>, which automatically download the data if they are missing:</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.breakfast import fetch_event_data\ndata, metadata = fetch_event_data(\nPath(\"/path/to/data/breakfast/\"), name=\"segmentation_coarse\"\n)\n</code></pre> <p>By default, the duplicate event sequences are removed. There are duplicate event sequences because there are multiple videos of the same scene. You can set <code>remove_duplicate=False</code> to keep the duplicate event sequences.</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.breakfast import load_event_data\ndata, metadata = load_event_data(\nPath(\"/path/to/data/breakfast/segmentation_coarse/\"), remove_duplicate=False\n)\nprint(data.summary())\nprint(data)\nprint(metadata)\n</code></pre> <p>The output should look like:</p> <pre><code>BatchDict(\n  (action_index) BatchedTensorSeq(dtype=torch.int64, shape=torch.Size([1712, 25]), device=cpu, batch_dim=0, seq_dim=1)\n  (cooking_activity) BatchList(batch_size=1712)\n  (end_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([1712, 25, 1]), device=cpu, batch_dim=0, seq_dim=1)\n  (person_id) BatchList(batch_size=1712)\n  (start_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([1712, 25, 1]), device=cpu, batch_dim=0, seq_dim=1)\n)\n\nBatchDict(\n  (action_index) tensor([[ 0, 16, 18,  ..., -1, -1, -1],\n            [ 0, 16, 18,  ..., -1, -1, -1],\n            [ 0, 16, 18,  ..., -1, -1, -1],\n            ...,\n            [ 0, 13, 20,  ..., -1, -1, -1],\n            [ 0, 13, 20,  ..., -1, -1, -1],\n            [ 0, 13, 20,  ..., -1, -1, -1]], batch_dim=0, seq_dim=1)\n  (cooking_activity) BatchList(data=['cereals', 'cereals', 'cereals', ..., 'tea'])\n  (end_time) tensor([[[ 30.],\n             [150.],\n             [428.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[ 30.],\n             [150.],\n             [428.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[ 30.],\n             [150.],\n             [428.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            ...,\n\n            [[ 37.],\n             [ 92.],\n             [229.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[ 37.],\n             [ 92.],\n             [229.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[ 37.],\n             [ 92.],\n             [229.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]]], batch_dim=0, seq_dim=1)\n  (person_id) BatchList(data=['P03', 'P03', 'P03', ..., 'P54'])\n  (start_time) tensor([[[  1.],\n             [ 31.],\n             [151.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[  1.],\n             [ 31.],\n             [151.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[  1.],\n             [ 31.],\n             [151.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            ...,\n\n            [[  1.],\n             [ 38.],\n             [ 93.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[  1.],\n             [ 38.],\n             [ 93.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]],\n\n            [[  1.],\n             [ 38.],\n             [ 93.],\n             ...,\n             [ nan],\n             [ nan],\n             [ nan]]], batch_dim=0, seq_dim=1)\n)\n\n{'action_vocab': Vocabulary(\n  counter=Counter({'SIL': 1016, 'pour_milk': 199, 'cut_fruit': 176, ..., 'stir_tea': 2}),\n  index_to_token=('SIL', 'pour_milk', 'cut_fruit', 'crack_egg', ..., 'stir_tea'),\n  token_to_index={'SIL': 0, 'pour_milk': 1, 'cut_fruit': 2, ..., 'stir_tea': 47},\n)}\n</code></pre> <p>It is possible to use the same function to get the event sequences with the fine annotations.</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.breakfast import load_event_data\ndata, metadata = load_event_data(Path(\"/path/to/data/breakfast/segmentation_fine/\"))\nprint(data)\nprint(metadata)\n</code></pre> <p>The output should look like:</p> <pre><code>BatchDict(\n  (action_index) BatchedTensorSeq(dtype=torch.int64, shape=torch.Size([257, 165]), device=cpu, batch_dim=0, seq_dim=1)\n  (cooking_activity) BatchList(batch_size=257)\n  (end_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([257, 165, 1]), device=cpu, batch_dim=0, seq_dim=1)\n  (person_id) BatchList(batch_size=257)\n  (start_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([257, 165, 1]), device=cpu, batch_dim=0, seq_dim=1)\n)\n\nBatchDict(\n  (action_index) tensor([[  0,  44,  49,  ...,  -1,  -1,  -1],\n            [  0,  83,  91,  ...,  -1,  -1,  -1],\n            [  0,  44,  49,  ...,  -1,  -1,  -1],\n            ...,\n            [  0, 117,  62,  ...,  -1,  -1,  -1],\n            [  0,  27,   1,  ...,  -1,  -1,  -1],\n            [  0, 117, 115,  ...,  -1,  -1,  -1]], batch_dim=0, seq_dim=1)\n  (cooking_activity) BatchList(data=['cereals', 'cereals', 'cereals', ..., 'tea'])\n  (end_time) tensor([[[53.],\n             [63.],\n             [80.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            [[45.],\n             [47.],\n             [92.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            [[17.],\n             [32.],\n             [45.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            ...,\n\n            [[ 7.],\n             [22.],\n             [41.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            [[52.],\n             [63.],\n             [92.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            [[26.],\n             [47.],\n             [85.],\n             ...,\n             [nan],\n             [nan],\n             [nan]]], batch_dim=0, seq_dim=1)\n  (person_id) BatchList(data=['P03', 'P04', 'P05', ..., 'P46'])\n  (start_time) tensor([[[ 1.],\n             [54.],\n             [64.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            [[ 1.],\n             [46.],\n             [48.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            [[ 1.],\n             [18.],\n             [33.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            ...,\n\n            [[ 1.],\n             [ 8.],\n             [23.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            [[ 1.],\n             [53.],\n             [64.],\n             ...,\n             [nan],\n             [nan],\n             [nan]],\n\n            [[ 1.],\n             [27.],\n             [48.],\n             ...,\n             [nan],\n             [nan],\n             [nan]]], batch_dim=0, seq_dim=1)\n)\n\n{'action_vocab': Vocabulary(\n  counter=Counter({'garbage': 774, 'move': 649, 'carry_knife': 403, ..., 'carry_capSalt': 3}),\n  index_to_token=('garbage', 'move', 'carry_knife', ..., 'carry_capSalt'),\n  token_to_index={'garbage': 0, 'move': 1, 'carry_knife': 2, ..., 'carry_capSalt': 177},\n)}\n</code></pre>"},{"location":"datasets/breakfast/#filter-the-data-by-dataset-split","title":"Filter the data by dataset split","text":"<p>The <code>load_event_data</code> function returns all the event sequence of the Breakfast dataset. A common operation is to separate the data by dataset splits. You can split the data by using the <code>filter_batch_by_dataset_split</code> function. The following example shows how to filter the data for the <code>train1</code> split.</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.breakfast import filter_batch_by_dataset_split, load_event_data\ndata, metadata = load_event_data(Path(\"/path/to/data/breakfast/segmentation_coarse/\"))\ndata_train = filter_batch_by_dataset_split(data, \"train1\")\nprint(data_train.summary())\n</code></pre> <p>The output should look like:</p> <pre><code>BatchDict(\n  (action_index) BatchedTensorSeq(dtype=torch.int64, shape=torch.Size([146, 165]), device=cpu, batch_dim=0, seq_dim=1)\n  (cooking_activity) BatchList(batch_size=146)\n  (end_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([146, 165, 1]), device=cpu, batch_dim=0, seq_dim=1)\n  (person_id) BatchList(batch_size=146)\n  (start_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([146, 165, 1]), device=cpu, batch_dim=0, seq_dim=1)\n)\n</code></pre> <p>Similarly, the following example shows how to filter the data for the <code>test1</code> split.</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.breakfast import filter_batch_by_dataset_split, load_event_data\ndata, metadata = load_event_data(Path(\"/path/to/data/breakfast/segmentation_coarse/\"))\ndata_test = filter_batch_by_dataset_split(data, \"test1\")\nprint(data_test.summary())\n</code></pre> <p>The output should look like:</p> <pre><code>BatchDict(\n  (action_index) BatchedTensorSeq(dtype=torch.int64, shape=torch.Size([111, 165]), device=cpu, batch_dim=0, seq_dim=1)\n  (cooking_activity) BatchList(batch_size=111)\n  (end_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([111, 165, 1]), device=cpu, batch_dim=0, seq_dim=1)\n  (person_id) BatchList(batch_size=111)\n  (start_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([111, 165, 1]), device=cpu, batch_dim=0, seq_dim=1)\n)\n</code></pre>"},{"location":"datasets/multithumos/","title":"MultiTHUMOS","text":""},{"location":"datasets/multithumos/#information-about-the-dataset","title":"Information about the dataset","text":"<p>The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.</p> <ul> <li>Project   page: http://ai.stanford.edu/~syyeung/everymoment.html</li> <li>Research paper: Yeung S., Russakovsky O., Jin N., Andriluka M., Mori G., Fei-Fei L.   Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos.   IJCV 2017 link</li> </ul>"},{"location":"datasets/multithumos/#download-data","title":"Download data","text":"<p><code>eternity</code> provides a functionality to download annotation data. The <code>download_annotations</code> function can be used to download automatically the annotation data. The following example shows how to download the annotation data in the path <code>/path/to/data/multithumos</code>.</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.multithumos import download_annotations\npath = Path(\"/path/to/data/multithumos\")\ndownload_annotations(path)\nprint(list(path.iterdir()))\n</code></pre> <p>The output should look like:</p> <pre><code>[PosixPath('/path/to/data/multithumos/README'),\n PosixPath('/path/to/data/multithumos/annotations'),\n PosixPath('/path/to/data/multithumos/class_list.txt')]\n</code></pre> <p>Note that is possible to download the data manually by following the instructions from the project webpage.</p> <p>The remaining of the documentation assumes the data are stored in the directory <code>/path/to/data/multithumos/</code>.</p>"},{"location":"datasets/multithumos/#action-prediction-task","title":"Action prediction task","text":"<p>This section explains how to prepare the data for the action prediction task.</p>"},{"location":"datasets/multithumos/#get-the-event-data","title":"Get the event data","text":"<p>After the data are downloaded, you can get the event sequences by using the <code>load_event_data</code> function. This function returns the data and metadata. The following example shows how to get the all the event sequences:</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.multithumos import load_event_data\ndata, metadata = load_event_data(Path(\"/path/to/data/multithumos/\"))\nprint(data)\nprint(metadata)\n</code></pre> <p>The output should look like:</p> <pre><code>BatchDict(\n  (video_id) BatchList(batch_size=413)\n  (action_index) BatchedTensorSeq(dtype=torch.int64, shape=torch.Size([413, 1235]), device=cpu, batch_dim=0, seq_dim=1)\n  (start_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([413, 1235]), device=cpu, batch_dim=0, seq_dim=1)\n  (end_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([413, 1235]), device=cpu, batch_dim=0, seq_dim=1)\n)\n\n{'action_vocab': Vocabulary(\n  counter=Counter({'BaseballPitch': 1, 'BasketballBlock': 1, 'BasketballDribble': 1, ...}),\n  index_to_token=('BaseballPitch', 'BasketballBlock', 'BasketballDribble', ...),\n  token_to_index={'BaseballPitch': 0, 'BasketballBlock': 1, 'BasketballDribble': 2, ...},\n)}\n</code></pre> <p>The MultiTHUMOS dataset has two official dataset splits: <code>val</code> and <code>test</code>. The following example shows how to get only the event sequences from the validation split.</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.multithumos import load_event_data\ndata, metadata = load_event_data(Path(\"/path/to/data/multithumos/\"), split=\"val\")\nprint(data)\nprint(metadata)\n</code></pre> <p>The output should look like:</p> <pre><code>BatchDict(\n  (video_id) BatchList(batch_size=200)\n  (action_index) BatchedTensorSeq(dtype=torch.int64, shape=torch.Size([200, 622]), device=cpu, batch_dim=0, seq_dim=1)\n  (start_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([200, 622]), device=cpu, batch_dim=0, seq_dim=1)\n  (end_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([200, 622]), device=cpu, batch_dim=0, seq_dim=1)\n)\n\n{'action_vocab': Vocabulary(\n  counter=Counter({'BaseballPitch': 1, 'BasketballBlock': 1, 'BasketballDribble': 1, ...}),\n  index_to_token=('BaseballPitch', 'BasketballBlock', 'BasketballDribble', ...),\n  token_to_index={'BaseballPitch': 0, 'BasketballBlock': 1, 'BasketballDribble': 2, ...},\n)}\n</code></pre> <p>The validation split should contain 200 sequences and the maximum sequence length is 622. The following example shows how to get only the event sequences from the test split.</p> <pre><code>from pathlib import Path\nfrom aroma.datasets.multithumos import load_event_data\ndata, metadata = load_event_data(Path(\"/path/to/data/multithumos/\"), split=\"test\")\nprint(data)\nprint(metadata)\n</code></pre> <p>The output should look like:</p> <pre><code>BatchDict(\n  (video_id) BatchList(batch_size=213)\n  (action_index) BatchedTensorSeq(dtype=torch.int64, shape=torch.Size([213, 1235]), device=cpu, batch_dim=0, seq_dim=1)\n  (start_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([213, 1235]), device=cpu, batch_dim=0, seq_dim=1)\n  (end_time) BatchedTensorSeq(dtype=torch.float32, shape=torch.Size([213, 1235]), device=cpu, batch_dim=0, seq_dim=1)\n)\n\n{'action_vocab': Vocabulary(\n  counter=Counter({'BaseballPitch': 1, 'BasketballBlock': 1, 'BasketballDribble': 1, ...}),\n  index_to_token=('BaseballPitch', 'BasketballBlock', 'BasketballDribble', ...),\n  token_to_index={'BaseballPitch': 0, 'BasketballBlock': 1, 'BasketballDribble': 2, ...},\n)}\n</code></pre> <p>The test split should contain 213 sequences and the maximum sequence length is 1235.</p>"}]}